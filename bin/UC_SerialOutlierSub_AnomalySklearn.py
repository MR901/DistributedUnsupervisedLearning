## Sklearn Anomaly Algorithm

import matplotlib.pyplot as plt
import numpy as np

import pandas as pd
import joblib, ast, time
from sklearn.ensemble import IsolationForest
from sklearn.covariance import EllipticEnvelope
from sklearn.neighbors import LocalOutlierFactor
from sklearn.svm import OneClassSVM
from UC_DataProcessing_GenMiniFunc import DataFrameScaling, GenerateCorrelationPlot, GeneralStats

from sklearn.metrics import confusion_matrix
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import RandomForestClassifier


def PlotingOutlierScoreOrError(ListContaingScore, config, ScoreThreshold = 0, title = 'Title'): 
    '''
    for ploting the Scores Generated by the Algorithms
    '''
    width = 20
    height = 7
    MinValue = min(ListContaingScore)
    MeanValue = sum(ListContaingScore)/len(ListContaingScore)
    MedianValue = np.median(ListContaingScore)
    MaxValue = max(ListContaingScore)
    ObsLessThanZero = str(sum([1 if(elem<ScoreThreshold) else 0 for elem in ListContaingScore]))
    ObsMoreThanZero = str(sum([1 if(elem>=ScoreThreshold) else 0 for elem in ListContaingScore]))
    
    fig = plt.figure(figsize=(width, height))
    plt.subplot(121)
    plt.plot([elem if elem >= ScoreThreshold else None for elem in ListContaingScore ], alpha = 0.8, color='k', linewidth=1, linestyle='-', marker='o', markerfacecolor='olive', markersize=7, label = '>='+str(ScoreThreshold))
    plt.plot([elem if elem < ScoreThreshold else None for elem in ListContaingScore ], alpha = 0.8, color='k', linewidth=1, linestyle='-', marker='o', markerfacecolor='brown', markersize=7, label = '<'+str(ScoreThreshold)) 
    #plt.plot(ListContaingScore, color='k', linewidth=1, linestyle='-', marker='o', markerfacecolor='black', markersize=8)
    #plt.title('Plotting Outlier Score generated from :'+ title, fontsize=14)
    plt.xlabel('Observation Index')
    plt.ylabel('Score')
    
    #     plt.xticks(NoOfFeature)
    # plt.xticks(np.arange(start=0, stop=17, step=1))
    # plt.yticks(np.arange(start=0, stop=501, step=100))
    #     plt.axis([0,1,0,100])
    
    plt.axhline(MinValue, color='k', ls = 'dotted', alpha = 0.9, lw = 4, label='Min Value')
    plt.axhline(MeanValue, color='red', ls = 'dotted', alpha = 0.9, lw = 4, label='Mean Value')
    plt.axhline(MedianValue, color='green', ls = 'dotted', alpha = 0.9, lw = 4, label='Median Value')
    plt.axhline(MaxValue, color='k', ls = 'dotted', alpha = 0.9, lw = 4, label='Max Value')
    
    plt.legend()
    plt.gca().invert_yaxis() ## inverting the axis
    plt.axhline(0, color='black') ## Margin Lines
    plt.axvline(0, color='black') ## Margin Lines
    #     plt.margins(1,1)
    
    plt.grid(True, color='black', alpha=0.2)

    
    plt.subplot(122)
    plt.hist([elem for elem in ListContaingScore if elem >= ScoreThreshold ], histtype = 'stepfilled', align = 'mid', color = 'olive', label = '>='+str(ScoreThreshold))#, bins=25
    plt.hist([elem for elem in ListContaingScore if elem < ScoreThreshold ], histtype = 'stepfilled', align = 'mid', color = 'brown', label = '<'+str(ScoreThreshold))#, bins=25
    #plt.hist(ListContaingScore, bins=25, histtype = 'stepfilled')
    plt.xlabel('Score')
    plt.ylabel('Count')
    
    plt.axvline(MinValue, color='k', ls = 'dotted', alpha = 0.9, lw = 4, label='Min Value')
    plt.axvline(MeanValue, color='red', ls = 'dotted', alpha = 0.9, lw = 4, label='Mean Value')
    plt.axvline(MedianValue, color='green', ls = 'dotted', alpha = 0.9, lw = 4, label='Median Value')
    plt.axvline(MaxValue, color='k', ls = 'dotted', alpha = 0.9, lw = 4, label='Max Value')
    
    plt.legend()
    plt.axhline(0, color='black') ## Margin Lines
    plt.axvline(0, color='black') ## Margin Lines
    #     plt.margins(1,1)
    
    plt.grid(True, color='black', alpha=0.2)
    
    plt.gcf().text(x=0.105, y=0.95, s = 'Plotting Outlier Score generated from : '+ title, fontsize=18, fontweight = 'bold')
    plt.gcf().text(x=0.12, y=0.9, s = '# of Observation Less than Zero '+ObsLessThanZero+' :: # of Observation More & Equal than Zero '+ObsMoreThanZero, fontsize=14, bbox=dict(facecolor='white', alpha=0.5))
    
    plt.show()
    fig.savefig(config['input']['ClustFileSavingLoc_dir'] + 'SerialOutlierDensity__{}.png'.format(time.time()), bbox_inches="tight")
    

def WriteSerialAnomalyOutputFile(Dataset, Algo, config_clust):
    DF = pd.DataFrame.copy(Dataset)
    
    if(len(Algo.split('Train_')) >= 2):
        DatasetType = 'Train_'
        Algo = Algo.split('Train_')[1]
    if(len(Algo.split('Test_')) >= 2):
        DatasetType = 'Test_'
        Algo = Algo.split('Test_')[1]
    FileName = Algo
    
    FeatureToIgnore = [ i for i in config_clust['DataProcessing_General']['FeatureToIgnore'].split("'") if len(i) > 2 ]
    
    Predict = DF.filter(like='Predict').columns
    Score = DF.filter(like='Score').columns
    
    if(len(Predict) == 0):
        DF[Algo+'_Predict'] = DF[DF.filter(like='Score').columns[0]].tolist()
    if(len(Score) == 0):
        DF[Algo+'_Score'] = DF[DF.filter(like='Predict').columns[0]].tolist()
        
    Predict = DF.filter(like='Predict').columns[0]
    Score = DF.filter(like='Score').columns[0]
    
    DF = DF[FeatureToIgnore + [Predict, Score]]
    
    FileName = DatasetType + Algo
    DF = DF.rename(columns = {Score: Algo+'__OutlierScore'})
    DF.to_csv(config_clust['input']['ClustFileSavingLoc_dir'] + 'SerialAnomaly' + '__' + FileName + '.csv', index=False)
    print('Results Saved Offline')
    

def PredictFunction_Alternate(x, y):
    '''
    To Be updated when Distributed Supervided Learning is Added to This Module -- Evaluation Cross Validation Mmodel Optimization
    -- Confusio mAtrix 
    '''
    X = x.copy()
    y = y.copy()
    
    print('Predict Alternate : X_Shape, y_Shape :', X.shape, y.shape)
    Mod_Class = RandomForestClassifier()
    Mod_Class.fit(X, y)
    
    Label_Pred = Mod_Class.predict(X).tolist()
    Label_True = y.tolist()
    
    tn, fp, fn, tp = confusion_matrix(Label_True, Label_Pred).ravel()
    print('Confusion Matrix\n\tTP:', tp, '\tFP:', fp, '\n\tFN:', fn, '\t\tTN:', tn)
    #print(regr.feature_importances_)
    
    return Mod_Class


def OutlierDetection_Sklearn(train_df, test_df, ClustAlgo, WhichParamsToUse, config_clust):
    """
    This is constructed in a way to handle the both formats of the Data, 'TrainTest' and 'GLTest'
    
    These Configuration based Action selection is ignored and Dataset is treated in a general way.
    
    ### Check for the Warnings That Are Appearing With EllipticalEnvelop
    """
    if(type(train_df) == pd.core.frame.DataFrame):
        trainDF = pd.DataFrame.copy(train_df)
        AllFeature = list(trainDF.columns)
    else:
        trainDF = None
    if(type(test_df) == pd.core.frame.DataFrame):
        testDF = pd.DataFrame.copy(test_df)
        AllFeature = list(testDF.columns)
    else:
        testDF = None
    config = config_clust
    FeatureToIgnore = ast.literal_eval(config_clust['DataProcessing_General']['FeatureToIgnore'])
    
    AlgoScoreInverseDict = ast.literal_eval(config['DataProcessing_Outlier']['InverseScoreScaleDirGivenOutlierHavingLeastScore'])
    ScoreScaleInverse = AlgoScoreInverseDict[ClustAlgo]['InverseScale']
    GraphScoreThreshold = AlgoScoreInverseDict[ClustAlgo]['GraphScoreThreshold']
    
    ## Pipeline Methodology
    # from sklearn.linear_model import LogisticRegression
    # from sklearn.pipeline import Pipeline
    # cvec = CountVectorizer()
    # lr = LogisticRegression()
    # vectorizer.set_params(stop_words=stop_words, max_features=n, ngram_range=ngram_range)
    # ug_pipeline = Pipeline([ ('vectorizer', cvec), ('classifier', lr) ])
    # sentiment_fit = pipeline.fit(x_train, y_train)
    # y_pred = sentiment_fit.predict(x_test)
    
    ### Defining Models and their property
    AnomalyDetectionModels_dict = {
        'IsolationForest': {'Model': IsolationForest(), 'DataTypeBoundation': 'Nil', 
                            'fit': True, 'fit_predict': False, 'predict': True, 
                            'DecisionFunction': True}, 
        'EllipticEnvelope': {'Model': EllipticEnvelope(), 'DataTypeBoundation': 'Normalized', 
                             'fit': True, 'fit_predict': False, 'predict': True, 
                             'DecisionFunction': True}, 
        'LocalOutlierFactor': {'Model': LocalOutlierFactor(), 'DataTypeBoundation': 'Nil', 
                               'fit': True, 'fit_predict': True, 'predict': False, 
                               'DecisionFunction': False}, 
        'OneClassSVM': {'Model': OneClassSVM(), 'DataTypeBoundation': 'Nil', 
                        'fit': True, 'fit_predict': False, 'predict': True, 
                        'DecisionFunction': True}, 
        # 'OneClassSVM': {'Model': OneClassSVM(**params), 'ModIdentify_type': 'AnomalyModelData', 'DataTypeBoundation': 'Nil'},
    }
    
    ## Getting the Configuration for the Algorithm
    #params = ast.literal_eval(config_clust['ML_AnomalyClusterConfiguration'][ClustAlgo + '_ParamConfig'])
    ## https://stackoverflow.com/questions/33110973/pass-a-dict-to-scikit-learn-estimator
    params =  ast.literal_eval(config['DataProcessing_Outlier'][WhichParamsToUse])
    
    Model = AnomalyDetectionModels_dict[ClustAlgo]['Model']
    Model.set_params(**params)
    ModelSpecificDataPreparation = AnomalyDetectionModels_dict[ClustAlgo]['DataTypeBoundation']
    
    ## Normalizing the Dataset for the algorithm which requires non negative dataset
    ### importing From External File
    FeatureScalingID = 'SerAnomRemovalFromMainPipeline__' + ClustAlgo + '_With_' + WhichParamsToUse  + '_DataPreparation_' + ModelSpecificDataPreparation
    if ModelSpecificDataPreparation != 'Nil':
        if(trainDF is not None):
            trainDF, _ = DataFrameScaling(trainDF, FeatureToIgnore, config_clust, FeatureScalingID, ModelSpecificDataPreparation)
        ## Test Should Not Change the Stored Values
        if(testDF is not None):
            testDF, _ = DataFrameScaling(testDF, FeatureToIgnore, config_clust, FeatureScalingID, ModelSpecificDataPreparation, 'GlTest')
            ## Only with this B/c of conceptual drift
            testDF_transformed = testDF[FeatureToIgnore].reset_index(drop=True)  
    
    
    print('Series Anomaly Removal Using', ClustAlgo)
    ModelName = config_clust['input']['ModelsSaving_dir'] + FeatureScalingID  

    ## Training Model
    # if(trainDF.shape[0] != 0):
    if(trainDF is not None):
        print('Developing and Saving Model :: Training Section :: On provided Training Data')
        
#         DataToScale = trainDF.loc[:, [ i for i in AllFeature if i not in FeatureToIgnore ]]
#         ## Handling the Error:: TypeError: Object of type 'int64' is not JSON serializable  ## Numpy ERROR
#         for col in DataToScale.dtypes[DataToScale.dtypes != 'float64'].index:
#             DataToScale[col] = DataToScale[col].astype('float64')
#         trainDF.loc[:, [ i for i in AllFeature if i not in FeatureToIgnore ]] = DataToScale
        
        if(AnomalyDetectionModels_dict[ClustAlgo]['fit_predict'] == True):
            trainDF[ClustAlgo + '_Predict'] = pd.DataFrame( Model.fit_predict(trainDF[[ j for j in AllFeature if j not in FeatureToIgnore ]]) )  
        elif((AnomalyDetectionModels_dict[ClustAlgo]['fit'] == True) & (AnomalyDetectionModels_dict[ClustAlgo]['predict'] == True)):
            Model.fit(trainDF[[ j for j in AllFeature if j not in FeatureToIgnore ]]) 
            trainDF[ClustAlgo + '_Predict'] = pd.DataFrame(Model.predict(trainDF[[ j for j in AllFeature if j not in FeatureToIgnore ]])) 
        else:
            print('Some Error is present')
        
        if(AnomalyDetectionModels_dict[ClustAlgo]['predict'] == False):
            X = trainDF[[ j for j in AllFeature if j not in FeatureToIgnore ]]
            y = trainDF[ClustAlgo + '_Predict']
            Mod = PredictFunction_Alternate(X,y)
            ## Saving the model used for predict function locally
            joblib.dump(Mod, ModelName+'_AddedExtPredict')
        
        ## Generating Score if it can be generated
        if(AnomalyDetectionModels_dict[ClustAlgo]['DecisionFunction'] == True):
            trainDF[ClustAlgo + '_Score'] = pd.DataFrame( Model.decision_function(trainDF[[ j for j in AllFeature if j not in FeatureToIgnore ]]) ) 
            if(AnomalyDetectionModels_dict[ClustAlgo]['predict'] == False):
                ## considering General Boundary as 0, if less tham this value --> anomaly 
                trainDF[ClustAlgo + '_PredBasedOnScore'] = [ 1 if elem >= 0 else -1 for elem in trainDF[ClustAlgo + '_Score'] ]
        
        ## Saving the model locally
        joblib.dump(Model, ModelName)
    
    
    ## Using Developed Model 
    if(testDF is not None):
        print('Using Saved Model :: Predict Section :: On provided Test Data')
        ## Loading the locally saved model
        Model = joblib.load(ModelName)
        
        if(AnomalyDetectionModels_dict[ClustAlgo]['predict'] == True):
            testDF[ClustAlgo + '_Predict'] = pd.DataFrame( Model.predict(testDF[[ j for j in AllFeature if j not in FeatureToIgnore ]]) ) 
        elif(AnomalyDetectionModels_dict[ClustAlgo]['predict'] == False):
            ## using the model used for predict function locally
            Model_pred = joblib.load(ModelName+'_AddedExtPredict')
            X = testDF[[ j for j in AllFeature if j not in FeatureToIgnore ]]
            testDF[ClustAlgo + '_Predict'] = Model_pred.predict(X)
        
        if(AnomalyDetectionModels_dict[ClustAlgo]['DecisionFunction'] == True):
            testDF[ClustAlgo + '_Score'] = pd.DataFrame( Model.decision_function(testDF[[ j for j in AllFeature if j not in FeatureToIgnore ]]) ) 
            if(AnomalyDetectionModels_dict[ClustAlgo]['predict'] == False):
                testDF[ClustAlgo + '_PredBasedOnScore'] = [ 1 if elem >= 0 else -1 for elem in testDF[ClustAlgo + '_Score'] ]
        
            
    
    ## Converting the Format to a Standard Variant of Serial Anomality Detection
    if(AnomalyDetectionModels_dict[ClustAlgo]['DecisionFunction'] == True):
        LikeVarToPlot = '_Score'
        if(trainDF is not None):
            trainDF[ClustAlgo+'_Score'] = [ -1 * elem if(ScoreScaleInverse is True) else elem for elem in trainDF[ClustAlgo+'_Score'] ] ## If Scale is to be inversed
        elif(testDF is not None):
            testDF[ClustAlgo+'_Score'] = [ -1 * elem if(ScoreScaleInverse is True) else elem for elem in testDF[ClustAlgo+'_Score'] ]
    else:
        LikeVarToPlot = '_Predict'
    if(type(trainDF) == pd.core.frame.DataFrame):
        FeatureToKeep = FeatureToIgnore + trainDF.filter(like='_Predict', axis=1).columns.tolist() + trainDF.filter(like='_Score', axis=1).columns.tolist()
        trainDF = trainDF.loc[:, FeatureToKeep]
        WriteSerialAnomalyOutputFile(trainDF, 'Train_'+ClustAlgo, config_clust)
        if config_clust['TriggerTheseFunctions']['PlotingOutlierScoreOrError'] != 'False':
            PlotingOutlierScoreOrError(trainDF[trainDF.filter(like=LikeVarToPlot, axis=1).columns[0]].tolist(), config_clust, ScoreThreshold = GraphScoreThreshold, title = ClustAlgo + ' Scores in Trainset')
    else:
        trainDF = None
    if(type(testDF) == pd.core.frame.DataFrame):
        FeatureToKeep = FeatureToIgnore + testDF.filter(like='_Predict', axis=1).columns.tolist() + testDF.filter(like='_Score', axis=1).columns.tolist()
        testDF = testDF.loc[:, FeatureToKeep]
        WriteSerialAnomalyOutputFile(testDF, 'Test_'+ClustAlgo, config_clust)
        if config_clust['TriggerTheseFunctions']['PlotingOutlierScoreOrError'] != 'False': 
            PlotingOutlierScoreOrError(testDF[testDF.filter(like=LikeVarToPlot, axis=1).columns[0]].tolist(), config_clust, ScoreThreshold = GraphScoreThreshold, title = ClustAlgo + ' Scores in Testset')
    else:
        testDF = None
    
#     VisualizeClusters_1(train_dimen_transf_df, DimRedAlgo=DimRed, ClusterAlgo=ClustAlgo, config=config_clust, ax={'ax1': 0,'ax2': 1,'ax3': 2}, extra_color=config_clust['data_processing']['Visual_Extra_Color'])
    return trainDF, testDF ## Serial Standard Version